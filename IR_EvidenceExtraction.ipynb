{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riddhiman-M/Fact-Extraction-and-Verification/blob/main/IR_EvidenceExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eogcK_IR26bk"
      },
      "source": [
        "# SECTION 0: Install packages if required"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAVKIeyq7GpM"
      },
      "source": [
        "Install necessary packages as required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-FZCq3R3EQ5",
        "outputId": "6998ffea-19e5-41f3-faca-fb9d4544a037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-xla==1.11\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl (152.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 152.9 MB 31 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 11 kB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0) (3.10.0.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.35.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.56.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Installing collected packages: google-api-python-client, torch-xla, torch, cloud-tpu-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.11\n",
            "    Uninstalling google-api-python-client-1.12.11:\n",
            "      Successfully uninstalled google-api-python-client-1.12.11\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\n",
            "earthengine-api 0.1.303 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-1.11.0 torch-xla-1.11\n"
          ]
        }
      ],
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iOjhthqrYsl",
        "outputId": "643efb76-19b1-4357-ce21-f8bdf878962a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDlYLe2srht3",
        "outputId": "5df39903-f260-40ce-fc36-7d672e1953c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▌                         | 10 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 30 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 40 kB 33.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149867 sha256=744546a5b90eb8c63c9aa72085639ff31e742bc1e3b7d85974bc57f8e7a86174\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n"
          ]
        }
      ],
      "source": [
        "pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtKGoL2xrPmh",
        "outputId": "f356050b-e3f0-4a7d-8dfb-441433c227cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting polyfuzz\n",
            "  Downloading polyfuzz-0.3.4-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from polyfuzz) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from polyfuzz) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from polyfuzz) (4.63.0)\n",
            "Collecting rapidfuzz>=0.13.1\n",
            "  Downloading rapidfuzz-2.0.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 16.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from polyfuzz) (1.0.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from polyfuzz) (0.11.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from polyfuzz) (1.21.5)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from polyfuzz) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from polyfuzz) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->polyfuzz) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->polyfuzz) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->polyfuzz) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->polyfuzz) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.2.2->polyfuzz) (3.10.0.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->polyfuzz) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.2->polyfuzz) (1.15.0)\n",
            "Collecting jarowinkler<1.1.0,>=1.0.2\n",
            "  Downloading jarowinkler-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2.post1->polyfuzz) (3.1.0)\n",
            "Installing collected packages: jarowinkler, rapidfuzz, polyfuzz\n",
            "Successfully installed jarowinkler-1.0.2 polyfuzz-0.3.4 rapidfuzz-2.0.8\n"
          ]
        }
      ],
      "source": [
        "pip install polyfuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huChOkFR7WzN",
        "outputId": "e8c519e4-d342-4300-b7b9-7bcb2c934b6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flair\n",
            "  Downloading flair-0.11-py3-none-any.whl (400 kB)\n",
            "\u001b[K     |████████████████████████████████| 400 kB 28.0 MB/s \n",
            "\u001b[?25hCollecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Collecting hyperopt>=0.2.7\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 39.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Collecting more-itertools~=8.8.0\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 24.2 MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 9.2 MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 757 kB/s \n",
            "\u001b[?25hCollecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.2)\n",
            "Collecting transformers>=4.0.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 16.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.2)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 38.1 MB/s \n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.11.0)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.63.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (1.21.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.14.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (1.3.0)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (2.6.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (0.16.0)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2021.10.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 51.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: gdown, mpld3, overrides, sqlitedict, langdetect, pptree, wikipedia-api\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9692 sha256=299901db8aeaf10be3b6ef06458803993ddbc974e8f97002375b281a69d648e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=b0f839f97dda07094f2f48ade53a7a4e28a52835b86e5c6bd05d4808c4d7c9b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=42789d254ec412b06f4c820ebc3a844a8ea2ee89cc993269bdbd2fe69acd054f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15736 sha256=43fca132b03e7cf83423b6fdf62d87e52c41a8f9dec596696cb72d07004eb546\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/dd/2e/0ed4a25cb73fc30c7ea8d10b50acb7226175736067e40a7ea3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=736b7b035f75d63248203890c241549c796f7e65a54c7cd66be20fd3dda7cf69\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=9e1fb7c63e68eadc9008cebe136e960d813cd0f9b9a883198d567d683a60dd92\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/e8/7d/a9c3c19b4722608a0d8b05a38c36bc3f230c43becd2a46794b\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13477 sha256=cb8a2d27346d7b9c7a26e94b209ef2e9a2ac6c340934a00e499afe0de8274580\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built gdown mpld3 overrides sqlitedict langdetect pptree wikipedia-api\n",
            "Installing collected packages: requests, pyyaml, importlib-metadata, tokenizers, sentencepiece, sacremoses, py4j, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, pptree, mpld3, more-itertools, langdetect, konoha, janome, hyperopt, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.3\n",
            "    Uninstalling importlib-metadata-4.11.3:\n",
            "      Successfully uninstalled importlib-metadata-4.11.3\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.12.0\n",
            "    Uninstalling more-itertools-8.12.0:\n",
            "      Successfully uninstalled more-itertools-8.12.0\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.4.1 deprecated-1.2.13 flair-0.11 ftfy-6.1.1 gdown-3.12.2 huggingface-hub-0.5.1 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.5 pyyaml-6.0 requests-2.27.1 sacremoses-0.0.49 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.0.0 tokenizers-0.11.6 transformers-4.18.0 wikipedia-api-0.5.4\n"
          ]
        }
      ],
      "source": [
        "pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjw3ernE_qRi",
        "outputId": "08d14f18-3d49-4600-b14f-6fb718d58613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: polyfuzz[flair] in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[flair]) (4.63.0)\n",
            "Requirement already satisfied: rapidfuzz>=0.13.1 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[flair]) (2.0.8)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[flair]) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[flair]) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[flair]) (1.21.5)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[flair]) (0.11.2)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[flair]) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[flair]) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[flair]) (1.1.0)\n",
            "Requirement already satisfied: flair>=0.7 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[flair]) (0.11)\n",
            "Collecting torch<1.7.1,>=1.4.0\n",
            "  Using cached torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (2019.12.20)\n",
            "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (4.4.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (0.5.1)\n",
            "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (4.6.5)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (6.1.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (0.1.95)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (1.2.13)\n",
            "Requirement already satisfied: hyperopt>=0.2.7 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (0.2.7)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (3.6.0)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (4.18.0)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (1.5.11)\n",
            "Requirement already satisfied: more-itertools~=8.8.0 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (8.8.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (0.8.9)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (0.4.2)\n",
            "Requirement already satisfied: gdown==3.12.2 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (3.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (2.8.2)\n",
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (0.5.4)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (1.0.9)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (2.0.0)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (0.3.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (4.2.6)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (0.3)\n",
            "Requirement already satisfied: pptree in /usr/local/lib/python3.7/dist-packages (from flair>=0.7->polyfuzz[flair]) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair>=0.7->polyfuzz[flair]) (3.6.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair>=0.7->polyfuzz[flair]) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair>=0.7->polyfuzz[flair]) (1.15.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair>=0.7->polyfuzz[flair]) (1.14.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair>=0.7->polyfuzz[flair]) (5.2.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair>=0.7->polyfuzz[flair]) (1.3.0)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair>=0.7->polyfuzz[flair]) (0.10.9.5)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair>=0.7->polyfuzz[flair]) (2.6.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair>=0.7->polyfuzz[flair]) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata<4.0.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konoha<5.0.0,>=4.0.0->flair>=0.7->polyfuzz[flair]) (3.10.1)\n",
            "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from konoha<5.0.0,>=4.0.0->flair>=0.7->polyfuzz[flair]) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair>=0.7->polyfuzz[flair]) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair>=0.7->polyfuzz[flair]) (3.10.0.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->polyfuzz[flair]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->polyfuzz[flair]) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->polyfuzz[flair]) (1.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->polyfuzz[flair]) (2018.9)\n",
            "Requirement already satisfied: jarowinkler<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from rapidfuzz>=0.13.1->polyfuzz[flair]) (1.0.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair>=0.7->polyfuzz[flair]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair>=0.7->polyfuzz[flair]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair>=0.7->polyfuzz[flair]) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair>=0.7->polyfuzz[flair]) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2.post1->polyfuzz[flair]) (3.1.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch<1.7.1,>=1.4.0->polyfuzz[flair]) (0.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair>=0.7->polyfuzz[flair]) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair>=0.7->polyfuzz[flair]) (0.11.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair>=0.7->polyfuzz[flair]) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair>=0.7->polyfuzz[flair]) (0.0.49)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->flair>=0.7->polyfuzz[flair]) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair>=0.7->polyfuzz[flair]) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair>=0.7->polyfuzz[flair]) (7.1.2)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0\n",
            "    Uninstalling torch-1.11.0:\n",
            "      Successfully uninstalled torch-1.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.0\n"
          ]
        }
      ],
      "source": [
        "pip install polyfuzz[flair]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H7rVoSi7rlG"
      },
      "source": [
        "\n",
        "# SECTION 1: Cleaning the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX9JS-tjg2JR"
      },
      "source": [
        "Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIqpLO1jcHxO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import string\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HL4g-16BMI5",
        "outputId": "1497fdee-a2b1-43cb-9f5e-0ee2cb8497b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFmhUJVZ6bFl"
      },
      "source": [
        "Import files from Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt2_-JQ8BUMK"
      },
      "source": [
        "Link for the Dataset:\n",
        "https://drive.google.com/drive/folders/10CQ_BADLLNgALyTwC_18gmYzWL58cpG-?usp=sharing\n",
        "\n",
        "The Google Drive folder is shared, just add it to the base directory of your Drive (\"/content/drive/MyDrive\") and the mounting will work fine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSZrDU-onyLM",
        "outputId": "1c54a05e-9299-4116-a019-0635e62fa84e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb7bg4uBg6wx"
      },
      "source": [
        "Function to read data from json file to a list. instance_num is used to read the first n data in the json."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnF4VVjxgWSe"
      },
      "outputs": [],
      "source": [
        "def load_dataset_json(path, instance_num = 1e7):\n",
        "    data = []\n",
        "    with open(path, 'r') as openfile:\n",
        "        #data = json.load(openfile)\n",
        "        for iline, line in enumerate(openfile.readlines()):\n",
        "            data.append(json.loads(line))\n",
        "            if iline + 1 >= instance_num:\n",
        "                break\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdCtpP4Qo5DH"
      },
      "source": [
        "Function to read data from Wikipedia articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RULlApiccLe1"
      },
      "outputs": [],
      "source": [
        "def load_wiki_json(path, instance_num = 1e5):\n",
        "    data = []\n",
        "    with open(path, 'r') as openfile:\n",
        "        #data = json.load(openfile)\n",
        "        for iline, line in enumerate(openfile.readlines()):\n",
        "            data.append(json.loads(line))\n",
        "            if iline + 1 >= instance_num:\n",
        "                break\n",
        "    for d in data:\n",
        "      d.pop('lines')            \n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0JC0NV3hKoV"
      },
      "source": [
        "Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SzukwSxcWlO",
        "outputId": "5b5df4ac-6262-4a80-ac9f-18b8d5955afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 89296, 'claim': 'Henry Spencer is played by a Greek actor.'}\n",
            "{'id': 78554, 'claim': 'John Ritter died in October.'}\n",
            "{'id': 83809, 'claim': '13 Reasons Why is the only television series of 2012 in the drama-mystery genre.'}\n",
            "{'id': 49758, 'claim': 'Playboy is a magazine.'}\n",
            "{'id': 22973, 'claim': 'Alternative metal is the genre in which Alice in Chains usually performs.'}\n",
            "{'id': 181494, 'claim': 'Sam Peckinpah directed The Wild Bunch.'}\n",
            "{'id': 161592, 'claim': \"The St. John's water dog is a breed of domestic dog that was first bred in Newfoundland.\"}\n",
            "{'id': 117342, 'claim': 'Horseshoe crabs are not used in fertilizer.'}\n",
            "{'id': 172204, 'claim': 'Sia (musician) has received an award presented by the cable channel MTV.'}\n",
            "{'id': 95552, 'claim': 'Artificial intelligence raises concern.'}\n"
          ]
        }
      ],
      "source": [
        "test_path = '/content/drive/MyDrive/data/fever-data/test.jsonl'\n",
        "test_data = load_dataset_json(path=test_path, instance_num=20)\n",
        "\n",
        "for sample in test_data[:10]:\n",
        "    print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fhe7SiBhPFc"
      },
      "source": [
        "Loading all wikipedia articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srJP3kl5ck6m",
        "outputId": "d131adf6-7a60-4a4f-83b7-a7c262fed5dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100 % done"
          ]
        }
      ],
      "source": [
        "wiki_data=[]\n",
        "os.chdir('/content/drive/MyDrive/data/wiki-pages')\n",
        "i=1\n",
        "for file in os.listdir():\n",
        "  print('\\r', math.floor(100*i/109), '% done', end = '')\n",
        "  wiki_data.extend(load_wiki_json(path = file))\n",
        "  i += 1\n",
        "os.chdir('..')  \n",
        "os.chdir('..')  \n",
        "wiki_data.remove(wiki_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JPyPQZjhSWi"
      },
      "source": [
        "Cleaning the claims to get tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT8tChhFdILq"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for sample in test_data:\n",
        "    tokens =  re.split(r'[ -,._]', sample['claim'])\n",
        "    tokens = list(filter(lambda token: token not in string.punctuation, tokens))\n",
        "    sample['token'] = [lemmatizer.lemmatize(w.lower()) for w in tokens if not w.lower() in stop_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTGxhsp76ukB"
      },
      "source": [
        "**Sample Output**: \n",
        "\n",
        "> Tokens obtained from the claims\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT0Hk2sUNx-u",
        "outputId": "8d3ed5cf-227a-4ad1-96d9-2b6917456db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['henry', 'spencer', 'played', 'greek', 'actor']\n",
            "['john', 'ritter', 'died', 'october']\n",
            "['13', 'reason', 'television', 'series', '2012', 'drama-mystery', 'genre']\n",
            "['playboy', 'magazine']\n",
            "['alternative', 'metal', 'genre', 'alice', 'chain', 'usually', 'performs']\n",
            "['sam', 'peckinpah', 'directed', 'wild', 'bunch']\n",
            "['st', 'john', 'water', 'dog', 'breed', 'domestic', 'dog', 'first', 'bred', 'newfoundland']\n",
            "['horseshoe', 'crab', 'used', 'fertilizer']\n",
            "['sia', 'musician', 'received', 'award', 'presented', 'cable', 'channel', 'mtv']\n",
            "['artificial', 'intelligence', 'raise', 'concern']\n"
          ]
        }
      ],
      "source": [
        "for sample in test_data[:10]:   \n",
        "    print(sample['token'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6Jn-vk1hZ1p"
      },
      "source": [
        "Cleaning the wiki articles to get tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3tSc_kRc6Xt",
        "outputId": "ab7b7d7f-17ff-459e-a5fa-16636310bf8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100 % done"
          ]
        }
      ],
      "source": [
        "n = len(wiki_data)\n",
        "i = 1\n",
        "prt = 0\n",
        "for sample in wiki_data:\n",
        "  if math.floor(100*(i/n)) == prt:\n",
        "    print('\\r', prt, '% done', end = '')\n",
        "    prt += 1\n",
        "  tokens = re.split(r'[_,–. ]', sample['id'])\n",
        "  tokens = list(filter(lambda token: token not in string.punctuation, tokens))\n",
        "  sample['token'] = [lemmatizer.lemmatize(w.lower()) for w in tokens if not w.lower() in stop_words]\n",
        "  i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkZoiHaH6-GI"
      },
      "source": [
        "**Sample Output**: \n",
        "\n",
        "> Tokens obtained from the wiki articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdnvW0TZc7JL",
        "outputId": "7566fd62-ed2d-4c67-e9f2-55649547626c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sin', 'sukju']\n",
            "['south', 'oroville', 'california']\n",
            "['southwest', 'golf', 'classic']\n",
            "['st', \"philip's\", 'cathedral', 'san', 'felipe']\n",
            "['st', \"bartholomew's\", 'church', 'chipping']\n",
            "['soulmate', '-lrb-disambiguation-rrb-']\n",
            "['spanish', 'flu', 'research']\n",
            "['society', 'cultural', 'anthropology']\n",
            "['skip', 'ewing']\n",
            "['somerset', 'academy', '-lrb-pembroke', 'pine', 'florida-rrb-']\n"
          ]
        }
      ],
      "source": [
        "for sample in wiki_data[:10]:\n",
        "    print(sample['token'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nSX_67I8VCp"
      },
      "source": [
        "# SECTION 2: Document Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUpYW9n3gTnc"
      },
      "source": [
        "Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMYas0I_hguY"
      },
      "outputs": [],
      "source": [
        "from polyfuzz.models import TFIDF\n",
        "from polyfuzz import PolyFuzz\n",
        "import fuzzywuzzy\n",
        "from fuzzywuzzy import fuzz\n",
        "from polyfuzz.models import Embeddings\n",
        "from flair.embeddings import TransformerWordEmbeddings\n",
        "from operator import length_hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHdkSeVw-Dhu"
      },
      "source": [
        "Setting up the thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4eA_Chb-DLg"
      },
      "outputs": [],
      "source": [
        "fuzz_threshold = 55\n",
        "similarity_threshold = 0.8\n",
        "num_doc = 10\n",
        "num_claim = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMC0nsJunwXt"
      },
      "source": [
        "Document Selection:\n",
        ">  Two models designed:\n",
        "* Tf - Idf\n",
        "* BERT\n",
        "\n",
        ">   Document selection done in 3 layers:\n",
        "1.   Layer 1: Filter out articles which has similarity < fuzz_threshold, using fuzzy matching\n",
        "2.   Layer 2: Get Tf-Idf similarity score\n",
        "3.   Layer 3: Get a similiraty score using a BERT model\n",
        "\n",
        "Each article which crosses the threshold are assigned a score considering the similarity obtained from all 3 layers. Top 10 (can be varied by the variable num_doc) articles are chosen for the next step.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uxIJsva-2iH"
      },
      "source": [
        "Designing the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgXEJAekexNC"
      },
      "outputs": [],
      "source": [
        "tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0)\n",
        "model1 = PolyFuzz(tfidf)\n",
        "embeddings = TransformerWordEmbeddings('bert-base-multilingual-cased')\n",
        "bert = Embeddings(embeddings, min_similarity=0)\n",
        "model2 = PolyFuzz(bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAxV-gjpG1ZZ"
      },
      "source": [
        "Randomly selecting 5 (variable num_claim) claims for which the evidences are retrieved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMjiEpPuGzOj"
      },
      "outputs": [],
      "source": [
        "res = random.sample(range(0, len(test_data)), num_claim)\n",
        "res = [0,5,7,17,19]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN3ILNQQ-5Qq"
      },
      "source": [
        "Document Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9ssH3cykZXB",
        "outputId": "2b9b26fe-480b-4196-b180-d0854c942303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting on claim:  Henry Spencer is played by a Greek actor.\n",
            " 100 % done\n",
            "\n",
            "Starting on claim:  Sam Peckinpah directed The Wild Bunch.\n",
            " 100 % done\n",
            "\n",
            "Starting on claim:  Horseshoe crabs are not used in fertilizer.\n",
            " 100 % done\n",
            "\n",
            "Starting on claim:  The Portland Trail Blazers have thrice gone to the NBA Finals.\n",
            " 100 % done\n",
            "\n",
            "Starting on claim:  Brazilian Girls is a group.\n",
            " 100 % done\n",
            "\n"
          ]
        }
      ],
      "source": [
        "document = []\n",
        "for k in res:\n",
        "    print('Starting on claim: ', test_data[k]['claim'])\n",
        "    tup=[]\n",
        "    n = len(wiki_data)\n",
        "    prt = 0\n",
        "    for i in range(n):\n",
        "      if math.floor(100*(i+1)/n) == prt:\n",
        "        print(\"\\r\",prt,'% done',end=\"\")\n",
        "        prt += 1\n",
        "      count = 0\n",
        "      similarity = fuzz.token_sort_ratio(test_data[k]['token'],wiki_data[i]['token'])\n",
        "      if similarity > fuzz_threshold:\n",
        "        model1.match(test_data[k]['token'],wiki_data[i]['token'])\n",
        "        l = length_hint(sample['token'])\n",
        "        count = round(similarity*l*0.01,1)\n",
        "        for p in model1.get_matches()['Similarity']:\n",
        "          if p > similarity_threshold:\n",
        "            count+=1\n",
        "        if count > round(similarity*l*0.01,1):\n",
        "          model2.match(test_data[k]['token'],wiki_data[i]['token'])\n",
        "          for p in model2.get_matches()['Similarity']:\n",
        "            if p > similarity_threshold:\n",
        "              count+=1\n",
        "        if count > 3:\n",
        "          tup.append((i,count)) \n",
        "    tup.sort(key = lambda x: x[1], reverse=True)\n",
        "    document.append(tup[:num_doc])\n",
        "    del tup\n",
        "    print('\\n')\n",
        "del model1\n",
        "del model2    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIbeePSJvLxn"
      },
      "source": [
        "Output for Section 2:\n",
        "\n",
        "**DOCUMENT RETRIEVAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DgwsYzlxWunl",
        "outputId": "cad42fcf-a433-457b-fdb2-b6bd0e77b8d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claim:  Henry Spencer is played by a Greek actor.\n",
            "(2625886, 7.2) \t\t Henry_Spencer_Ashbee\n",
            "(2638145, 7.2) \t\t Henry_C._Spencer\n",
            "(2660251, 7.2) \t\t Henry_E._Spencer\n",
            "(3469927, 7.2) \t\t Henry_Spencer_Palmer\n",
            "(3566599, 7.2) \t\t Lord_Henry_Spencer\n",
            "(4560605, 7.1) \t\t Henry_Spencer_Berkeley\n",
            "(3096802, 6.9) \t\t List_of_actors_who_have_played_Sherlock_Holmes\n",
            "(4039115, 6.9) \t\t List_of_Greek_actors\n",
            "(4542972, 6.9) \t\t Henry_Elvins_Spencer\n",
            "(4555676, 6.9) \t\t Henry_Spencer\n",
            "\n",
            "\n",
            "Claim:  Sam Peckinpah directed The Wild Bunch.\n",
            "(3974591, 6.9) \t\t Sam_Peckinpah_bibliography\n",
            "(4004022, 6.9) \t\t Sam_Peckinpah\n",
            "(5278822, 6.9) \t\t Butch_Cassidy's_Wild_Bunch\n",
            "(4171832, 4.9) \t\t Bueng_Sam_Phan_District\n",
            "\n",
            "\n",
            "Claim:  Horseshoe crabs are not used in fertilizer.\n",
            "(3937879, 7.2) \t\t Horseshoe_crab\n",
            "(3844797, 7.0) \t\t Mangrove_horseshoe_crab\n",
            "(720281, 6.9) \t\t Atlantic_horseshoe_crab\n",
            "(1603546, 5.5) \t\t Battle_of_the_House_in_the_Horseshoe\n",
            "(3502453, 5.5) \t\t Horseshoe_Crater\n",
            "(3776655, 5.2) \t\t Darling's_horseshoe_bat\n",
            "(3803243, 5.2) \t\t Dent's_horseshoe_bat\n",
            "(4561068, 5.2) \t\t Hill's_horseshoe_bat\n",
            "(98708, 5.1) \t\t Decken's_horseshoe_bat\n",
            "(961523, 5.1) \t\t Fertilizer_subsidies_in_Sub-Saharan_Africa\n",
            "\n",
            "\n",
            "Claim:  The Portland Trail Blazers have thrice gone to the NBA Finals.\n",
            "(3375142, 9.5) \t\t Portland_Trail_Blazers_Radio_Network\n",
            "(3931100, 9.4) \t\t History_of_the_Portland_Trail_Blazers\n",
            "(3076135, 9.3) \t\t List_of_Portland_Trail_Blazers_seasons\n",
            "(3393476, 9.3) \t\t Portland_Trail_Blazers\n",
            "(3068489, 9.2) \t\t List_of_Portland_Trail_Blazers_head_coaches\n",
            "(3166655, 9.2) \t\t Portland_Trail_Blazers_all-time_roster\n",
            "(3176943, 9.2) \t\t Portland_Trail_Blazers_draft_history\n",
            "(2397244, 9.1) \t\t List_of_Portland_Trail_Blazers_executives\n",
            "(3219012, 8.9) \t\t 2010–11_Portland_Trail_Blazers_season\n",
            "(3220249, 8.9) \t\t 2017–18_Portland_Trail_Blazers_season\n",
            "\n",
            "\n",
            "Claim:  Brazilian Girls is a group.\n",
            "(574703, 8.1) \t\t Brazilian_Girls\n",
            "(2299492, 7.5) \t\t The_Best_of_the_Girl_Groups\n",
            "(562506, 7.4) \t\t Brazilian_Girls_-LRB-album-RRB-\n",
            "(3143620, 7.4) \t\t Girl_group\n",
            "(5329132, 6.9) \t\t List_of_girl_groups\n",
            "(4711711, 6.8) \t\t Confessions_of_a_Brazilian_Call_Girl\n",
            "(581269, 5.9) \t\t Brazilian_Gold_Rush\n",
            "(1541416, 5.8) \t\t Brazilian_hip_hop\n",
            "(580938, 5.6) \t\t Brazilian_rock\n",
            "(586825, 5.6) \t\t Brazilian_real\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "for doc in document:\n",
        "  if i>=num_claim:\n",
        "    break\n",
        "  print(\"Claim: \",test_data[res[i]]['claim'])\n",
        "  for d in doc:\n",
        "    print(d,'\\t\\t',wiki_data[d[0]]['id'])\n",
        "  print('\\n')\n",
        "  i+=1  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCLGlj_m_Gz_"
      },
      "source": [
        "# SECTION 3: Sentence Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtIhne89_GI1"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UBhIaTigZLY_"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMMk8pBY_gjY"
      },
      "source": [
        "Setting number of evidence sentences required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Hxn1MLdr_cvS"
      },
      "outputs": [],
      "source": [
        "num_sentence = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7iEIDxx_UnG"
      },
      "source": [
        "Results obtained from normal string matching without any model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wyrYc7f-YXI9",
        "outputId": "f91eab81-6133-4295-fde5-a48f480d481b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claim:  Henry Spencer is played by a Greek actor.\n",
            "Evidence: \n",
            "This is a list of Greek actors .\n",
            "Henry Spencer -LRB- born 1955 -RRB- is a Canadian computer programmer and space enthusiast .\n",
            "The list of actors who have played Sherlock Holmes in film , television , stage , or radio includes :\n",
            "He is coauthor , with David Lawrence , of the book Managing Usenet .\n",
            "Spencer was succeeded as mayor by Mark P. Taylor in 1851 .\n",
            "\n",
            "\n",
            "Claim:  Sam Peckinpah directed The Wild Bunch.\n",
            "Evidence: \n",
            "A list of books and essays about Sam Peckinpah :   Peckinpah\n",
            "Peckinpah 's combative personality , marked by years of alcohol and drug abuse , affected his professional legacy .\n",
            "He was given the nickname `` Bloody Sam '' owing to the violence in his films .\n",
            "It was popularized by the 1969 movie , Butch Cassidy and the Sundance Kid , and took its name from the original Wild Bunch .\n",
            "Peckinpah 's films generally deal with the conflict between values and ideals , and the corruption of violence in human society .\n",
            "\n",
            "\n",
            "Claim:  Horseshoe crabs are not used in fertilizer.\n",
            "Evidence: \n",
            "There are four extant species of horseshoe crab .\n",
            "They are commonly used as bait and in fertilizer .\n",
            "All four extant species of horseshoe crabs are anatomically very similar .\n",
            "The other three extant species in the family Limulidae are also called horseshoe crabs .\n",
            "Horseshoe crabs are marine arthropods of the family Limulidae and order Xiphosura or Xiphosurida .\n",
            "\n",
            "\n",
            "Claim:  The Portland Trail Blazers have thrice gone to the NBA Finals.\n",
            "Evidence: \n",
            "This is a list of Portland Trail Blazers executives , since the team 's foundation in 1970 .\n",
            "Canales was named interim coach of the Trail Blazers toward the end of the season .\n",
            "There have been 14 head coaches for the Trail Blazers franchise .\n",
            "The Trail Blazers are owned by Paul Allen , and Neil Olshey is their general manager .\n",
            "The Trail Blazers first participated in the NBA Draft on March 23 , 1970 , before their inaugural NBA season .\n",
            "\n",
            "\n",
            "Claim:  Brazilian Girls is a group.\n",
            "Evidence: \n",
            "Brazilian Girls is the first album by Brazilian Girls .\n",
            "Brazilian hip hop is a national music genre .\n",
            "This is a list of girl groups of all musical genres .\n",
            "Minas Gerais was the gold mining center of Brazil .\n",
            "reais -RRB- is the present-day currency of Brazil .\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "for doc in document:\n",
        "  if i>=num_claim:\n",
        "    break\n",
        "  print('Claim: ',test_data[res[i]]['claim'])\n",
        "  print('Evidence: ')\n",
        "  evidence=[]\n",
        "  for d in doc:\n",
        "    token_text = sent_tokenize(wiki_data[d[0]]['text'])\n",
        "    for s in token_text:\n",
        "      t = re.split(r'[_,–. ]', s)\n",
        "      t = list(filter(lambda token: token not in string.punctuation, t))\n",
        "      tok = [lemmatizer.lemmatize(w.lower()) for w in t if not w.lower() in stop_words]\n",
        "      similarity = fuzz.token_sort_ratio(test_data[res[i]]['token'],tok)+d[1]\n",
        "      evidence.append((s,similarity))\n",
        "  evidence.sort(key = lambda x: x[1], reverse=True)\n",
        "  if len(evidence) < 1:\n",
        "    print('Not enough evidence')\n",
        "  for e in evidence[:num_sentence]:\n",
        "    print(e[0])\n",
        "  print('\\n')\n",
        "  i+=1  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncj0KZPj__Vv"
      },
      "source": [
        "Sentence Selection:\n",
        ">  Two models designed:\n",
        "* Tf - Idf\n",
        "* BERT\n",
        "\n",
        ">   Sentence selection done in 2 layers:\n",
        "1.   Layer 1: Get Tf-Idf similarity score\n",
        "2.   Layer 2: Get a similiraty score using a BERT model\n",
        "\n",
        "Each claim - candidate sentence pair passed thorough both the layers. Top 5 (can be varied by the variable num_sentences) evidence sentences are chosen as the final result.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPpYeEh8_yiR"
      },
      "source": [
        "Design the 2 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lFj3NtVOYtPG"
      },
      "outputs": [],
      "source": [
        "embeddings = TransformerWordEmbeddings('bert-base-multilingual-cased')\n",
        "bert = Embeddings(embeddings, min_similarity=0)\n",
        "model4 = PolyFuzz(bert)\n",
        "tfidf = TFIDF(n_gram_range=(3, 3))\n",
        "model3 = PolyFuzz(tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLkboz4kAfex"
      },
      "source": [
        "## Final Output from the Project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XvJuG5xpkgYB",
        "outputId": "3d8459b0-832d-462a-82c8-eb8777d106c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "######################\n",
            "Claim:  Henry Spencer is played by a Greek actor.\n",
            "Evidence: \n",
            "Henry Spencer Ashbee -LRB- 21 April 1834 -- 29 July 1900 -RRB- was a book collector , writer , and bibliographer .\n",
            "Henry Christian Spencer -LRB- 1915 -- 2000 -RRB- was an American chemical engineer and executive at the Kerite Company in Seymour , Connecticut .\n",
            "Henry Evans Spencer -LRB- born June 13 , 1807 in Columbia - now part of Cincinnati -RRB- was a notable Cincinnati resident and was Mayor of Cincinnati from 1843-1851 .\n",
            "Major General Henry Spencer Palmer -LRB- 30 April 1838 -- 10 February 1893 -RRB- was a British army military engineer and surveyor , noted for his work in developing Yokohama harbor in the Empire of Japan as a foreign advisor to the Japanese government\n",
            "Lord Henry John Spencer -LRB- 20 December 1770 -- 3 July 1795 -RRB- was a British diplomat and politician .\n",
            "\n",
            "\n",
            "######################\n",
            "Claim:  Sam Peckinpah directed The Wild Bunch.\n",
            "Evidence: \n",
            "David Samuel `` Sam '' Peckinpah -LRB- -LSB- ˈpɛkɪnˌpɑː -RSB- February 21 , 1925 -- December 28 , 1984 -RRB- was an American film director and screenwriter who achieved prominence following the release of the Western epic The Wild Bunch -LRB- 1969 -RRB- .\n",
            "A list of books and essays about Sam Peckinpah :   Peckinpah\n",
            "Butch Cassidy 's Wild Bunch was one of the loosely organized outlaw gangs operating out of the Hole-in-the-Wall in Wyoming during the Old West era in the United States .\n",
            "It was popularized by the 1969 movie , Butch Cassidy and the Sundance Kid , and took its name from the original Wild Bunch .\n",
            "Peckinpah 's films generally deal with the conflict between values and ideals , and the corruption of violence in human society .\n",
            "\n",
            "\n",
            "######################\n",
            "Claim:  Horseshoe crabs are not used in fertilizer.\n",
            "Evidence: \n",
            "Horseshoe crabs are marine arthropods of the family Limulidae and order Xiphosura or Xiphosurida .\n",
            "Horseshoe crabs live primarily in and around shallow ocean waters on soft sandy or muddy bottoms .\n",
            "They are commonly used as bait and in fertilizer .\n",
            "Because of their origin 450 million years ago , horseshoe crabs are considered living fossils .\n",
            "The mangrove horseshoe crab -LRB- Carcinoscorpius rotundicauda -RRB- is a chelicerate arthropod found in marine and brackish waters .\n",
            "\n",
            "\n",
            "######################\n",
            "Claim:  The Portland Trail Blazers have thrice gone to the NBA Finals.\n",
            "Evidence: \n",
            "Jack Ramsay , who was the Trail Blazers head coach from 1976 to 1986 , had the number 77 retired in honor of Portland 's only NBA Finals victory in 1977 .\n",
            "The Portland Trail Blazers Radio Network is an American radio network consisting of 18 radio stations which carry coverage of the Portland Trail Blazers , a professional basketball team in the NBA .\n",
            "Ramsay is the only coach to win an NBA championship with the Trail Blazers , in the 1977 NBA Finals .\n",
            "The following is a list of players , both past and current , who appeared in at least one game for the Portland Trail Blazers NBA franchise .\n",
            "The 2010 -- 11 Portland Trail Blazers season was the 41st season of the franchise in the National Basketball Association -LRB- NBA -RRB- .\n",
            "\n",
            "\n",
            "######################\n",
            "Claim:  Brazilian Girls is a group.\n",
            "Evidence: \n",
            "Brazilian Girls is a band from New York City known for their eclectic blend of electronic dance music with musical styles as diverse as tango , chanson , house , reggae and lounge -LRB- but no Brazilian rhythms at all -RRB- .\n",
            "The Best of the Girl Groups is a 2-volume compilation series released by Rhino Records in 1990 .\n",
            "The collection , compiling 36 of the better known tracks by girl groups of the 1960s , shares spot # 422 in  Rolling Stones list of `` Greatest Albums of All Time '' .\n",
            "The New York Times recommends both volumes , in conjunction with Rhino 's Girl Group Greats , for listeners seeking `` the biggest girl-group hits '' .\n",
            "Brazilian Girls is the first album by Brazilian Girls .\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "k = 0\n",
        "for doc in document:\n",
        "  if k>=num_claim:\n",
        "    break\n",
        "  print('######################')\n",
        "  print('Claim: ',test_data[res[k]]['claim'])\n",
        "  print('Evidence: ')\n",
        "  evidence=[]\n",
        "  for d in doc:\n",
        "    token_text = sent_tokenize(wiki_data[d[0]]['text'])\n",
        "    for s in token_text:\n",
        "      t = re.split(r'[_,–. ]', s)\n",
        "      t = list(filter(lambda token: token not in string.punctuation, t))\n",
        "      tok = [lemmatizer.lemmatize(w.lower()) for w in t if not w.lower() in stop_words]\n",
        "      if len(tok) < 2:\n",
        "        continue\n",
        "      model3.match(test_data[res[k]]['token'],tok)\n",
        "      count = 0\n",
        "      l = length_hint(test_data[res[k]]['token'])\n",
        "      for i in model3.get_matches()['Similarity']:\n",
        "        if i > similarity_threshold:\n",
        "          count+=i\n",
        "      if count>0:\n",
        "        model4.match(test_data[res[k]]['token'],tok)\n",
        "        for i in model4.get_matches()['Similarity']:\n",
        "          if i > similarity_threshold:\n",
        "            count+=i\n",
        "      evidence.append((s,count))\n",
        "  evidence.sort(key = lambda x: x[1], reverse=True)\n",
        "  if len(evidence) < 1:\n",
        "    print('Not enough evidence\\n')\n",
        "    k+=1\n",
        "    continue\n",
        "  for e in evidence[:num_sentence]:\n",
        "    print(e[0])\n",
        "  print('\\n')\n",
        "  k+=1  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}